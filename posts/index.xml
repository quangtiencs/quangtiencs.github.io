<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on quangtiencs</title>
    <link>https://quangtiencs.com/posts/</link>
    <description>Recent content in Posts on quangtiencs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>quangtiencs ➤ bet on myself &amp; beat the odds</copyright>
    <lastBuildDate>Sun, 23 Apr 2023 08:00:00 +0700</lastBuildDate><atom:link href="https://quangtiencs.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[playground] Julia Turing.jl : Bayesian Cognitive Modeling - Some examples of data analysis</title>
      <link>https://quangtiencs.com/posts/turing_julia_bcm_chapter_005/</link>
      <pubDate>Sun, 23 Apr 2023 08:00:00 +0700</pubDate>
      
      <guid>https://quangtiencs.com/posts/turing_julia_bcm_chapter_005/</guid>
      <description>Github: https://github.com/quangtiencs/learning_notebook/blob/main/bcm-bayesian-cognitive-modeling/turing.jl-julia/ParameterEstimation/DataAnalysis.ipynb
using Logging using DynamicPPL, Turing using Zygote, ReverseDiff using StatsPlots, Random using LaTeXStrings using CSV using DataFrames using SpecialFunctions using LinearAlgebra using FillArrays using CSV, DataFrames Random.seed!(6) format=:svg 5.1 Pearson correlation $$ \mu_{1},\mu_{2} \sim \text{Gaussian}(0, 1/ \sqrt{.001}) $$ $$ \sigma_{1},\sigma_{2} \sim \text{InvSqrtGamma} (.001, .001) $$ $$ r \sim \text{Uniform} (-1, 1) $$
$$ x_{i} \sim \text{MvGaussian} \left( (\mu_{1},\mu_{2}), \begin{bmatrix}\sigma_{1}^2 &amp;amp; r\sigma_{1}\sigma_{2}\r\sigma_{1}\sigma_{2} &amp;amp; \sigma_{2}^2\end{bmatrix} \right) $$
x = [[0.</description>
    </item>
    
    <item>
      <title>[playground] Julia Turing.jl : Bayesian Cognitive Modeling - Inferences with gaussians</title>
      <link>https://quangtiencs.com/posts/turing_julia_bcm_chapter_004/</link>
      <pubDate>Wed, 22 Mar 2023 08:00:00 +0700</pubDate>
      
      <guid>https://quangtiencs.com/posts/turing_julia_bcm_chapter_004/</guid>
      <description>Github: https://github.com/quangtiencs/learning_notebook/blob/main/bcm-bayesian-cognitive-modeling/turing.jl-julia/ParameterEstimation/Gaussian.ipynb
using DynamicPPL, Turing using StatsPlots, Random using LaTeXStrings using CSV using DataFrames using SpecialFunctions format=:png 4.1 Inferring a mean and standard deviation $$ \mu \sim \text{Gaussian}(0, \sqrt{1000}) $$ $$ \sigma \sim \text{Uniform} (0, 10) $$ $$ x_{i} \sim \text{Gaussian} (\mu, \sigma^2) $$
x = [1.1, 1.9, 2.3, 1.8] @model function GaussianModel(x) mu ~ Normal(0, sqrt(1000)) sigma ~ Uniform(0, 10.0) for i in eachindex(x) x[i] ~ Normal(mu, sigma) end end iterations=10_000 chain = sample(GaussianModel(x), NUTS(2000, 0.</description>
    </item>
    
    <item>
      <title>[playground] Julia Turing.jl : Bayesian Cognitive Modeling - Inferences with binomials</title>
      <link>https://quangtiencs.com/posts/turing_julia_bcm_chapter_003/</link>
      <pubDate>Sat, 18 Mar 2023 08:00:00 +0700</pubDate>
      
      <guid>https://quangtiencs.com/posts/turing_julia_bcm_chapter_003/</guid>
      <description>Github: https://github.com/quangtiencs/learning_notebook/blob/main/bcm-bayesian-cognitive-modeling/turing.jl-julia/ParameterEstimation/Binomial.ipynb
3.1 Inferring a rate $$ \theta \sim \text{Beta}(1, 1) $$ $$ k \sim \text{Binomial} ( \theta, n) $$
using DynamicPPL, Turing using StatsPlots, Random using LaTeXStrings using CSV using DataFrames using SpecialFunctions k = 5 n = 10 @model function BinomialModel(k) theta ~ Beta(1, 1) k ~ Binomial(n, theta) end iterations = 1_000 ϵ = 0.05 τ = 10 chain = sample(BinomialModel(k), HMC(ϵ, τ), iterations) p = histogram(chain[:theta], label=L&#34;</description>
    </item>
    
    <item>
      <title>Probabilistic Programming 2023</title>
      <link>https://quangtiencs.com/posts/probabilistic_programming_2023/</link>
      <pubDate>Fri, 10 Feb 2023 08:00:00 +0700</pubDate>
      
      <guid>https://quangtiencs.com/posts/probabilistic_programming_2023/</guid>
      <description>Probabilistic Programming 2023, some libraries that I recently used:
PyMC: well-designed API with concise documentation. It&amp;rsquo;s faster and more customizable than before because the predecessor (the Theno backend) was replaced by the Aesara backend (compiling to C and Jax). BlackJax: for hacking log-density lovers. There&amp;rsquo;re some new algorithms like Stochastic gradient Langevin dynamics. BlackJax is not a complete probabilistic programming language. It integrates well with the PPLs backend by Jax. Stan: a domain-specific language for statistical modeling and one of the fastest samplers.</description>
    </item>
    
    <item>
      <title>[playground] Tensorflow.Js &amp; Typescript [4]: Quantile Regression</title>
      <link>https://quangtiencs.com/posts/tensorflowjs_typescript_04/</link>
      <pubDate>Mon, 23 Jan 2023 08:00:00 +0700</pubDate>
      
      <guid>https://quangtiencs.com/posts/tensorflowjs_typescript_04/</guid>
      <description>Quantile Regression is one of the practical techniques for many real problems. In this tutorial, we will implement a custom loss for TensorflowJS.
Quantile Loss:
$$\mathcal{L}(y_{\mathtt{true}}, y_{\mathtt{pred}}) = \begin{cases} (y_{\mathtt{true}} - y_{\mathtt{pred}}) \alpha &amp;amp;\text{if } y_{\mathtt{true}} \ge y_{\mathtt{pred}} \\ (y_{\mathtt{true}} - y_{\mathtt{pred}}) (\alpha - 1) &amp;amp;\text{if } y_{\mathtt{true}} &amp;lt; y_{\mathtt{pred}} \end{cases} $$
Or for easy computing:
$$\mathcal{L}(y_{\mathtt{true}}, y_{\mathtt{pred}}) = \mathtt{max}((y_{\mathtt{true}} - y_{\mathtt{pred}}) \alpha, (y_{\mathtt{true}} - y_{\mathtt{pred}}) (\alpha - 1)) $$</description>
    </item>
    
    <item>
      <title>[playground] Tensorflow.Js &amp; Typescript [3]: Modeling</title>
      <link>https://quangtiencs.com/posts/tensorflowjs_typescript_03/</link>
      <pubDate>Sun, 22 Jan 2023 20:00:00 +0700</pubDate>
      
      <guid>https://quangtiencs.com/posts/tensorflowjs_typescript_03/</guid>
      <description>Tensorflow.JS provides an application programming interface similar to Tensorflow (Python API). Although it has few choices (layers, models, optimizers), it is still helpful in some applications that need online learning on client devices.
1. APIs: Sometimes useful:
Layers API: tf.layers.dense, tf.layers.dropout, tf.layers.embedding, tf.layers.dense (elu, hardSigmoid, linear, relu, relu6, selu, sigmoid, softmax, softplus, softsign, tanh, swish, mish), Model API: tf.sequential, tf.model. Build-In Optimizers: sgd, adagrad, adadelta, adam, adamax, rmsprop. Build-In Loss functions: tf.</description>
    </item>
    
    <item>
      <title>[playground] Tensorflow.Js &amp; Typescript [2]: Memory management</title>
      <link>https://quangtiencs.com/posts/tensorflowjs_typescript_02/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0530</pubDate>
      
      <guid>https://quangtiencs.com/posts/tensorflowjs_typescript_02/</guid>
      <description>Management Memory is essential for every program to work efficiently. Although Javascript has a Garbage Collector, our programs with TensorflowJS don&amp;rsquo;t get the same automatic memory management.
The tensor objects are persistent with the memory, although the javascript variable has no reference. This lead to memory leak problem.
Let&amp;rsquo;s understand the problem deeper through examples!
1. Memory information: Sometimes you need to get your memory information, and these functions are helpful:</description>
    </item>
    
    <item>
      <title>[playground] Tensorflow.Js &amp; Typescript [1]: Quick Start</title>
      <link>https://quangtiencs.com/posts/tensorflowjs_typescript_01/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0530</pubDate>
      
      <guid>https://quangtiencs.com/posts/tensorflowjs_typescript_01/</guid>
      <description>TensorFlow is one of the most well-known libraries for machine learning. The most significant advantage of TensorFlow versus other libraries is designed to simplify the development of cross-platform projects.
These days, TensorFlow.js (Javascript) is a sub-project of Tensorflow that support three environments: Node.js, Web browser, and Mobile (via React Native). Unfortunately, Javascript is not a good programming language for data science projects because it is easy to make mistakes with little experience.</description>
    </item>
    
    <item>
      <title>Julia Language - Euler Project</title>
      <link>https://quangtiencs.com/posts/julia_euler_project/</link>
      <pubDate>Sun, 10 Jul 2022 00:00:00 +0530</pubDate>
      
      <guid>https://quangtiencs.com/posts/julia_euler_project/</guid>
      <description>Github: https://github.com/quangtiencs/julia_project_euler Project Euler 8: Largest product in a series HackerRank: https://www.hackerrank.com/contests/projecteuler/challenges/euler008 Euler: https://projecteuler.net/problem=8 function greatest_product_of_consecutive_digits(array::Array{Int}, k::Int)::Int product = prod(array[1:k]) cache = array[1] maximum_prod = product for i in 1:(length(array)-k) if cache != 0 product = div(product, cache) * array[k+i] else product = prod(array[1+i:k+i]) end cache = array[1+i] if product &amp;gt; maximum_prod maximum_prod = product end end return maximum_prod end function main() t = parse(Int64, readline()) for i in 1:t n, k = map((x) -&amp;gt; parse(Int64, x), split(readline(), &amp;quot; &amp;quot;)) arr_number = [parse(Int, e) for e in readline()] result = greatest_product_of_consecutive_digits(arr_number, k) println(result) end end main() Project Euler 7: 10001st prime HackerRank: https://www.</description>
    </item>
    
    <item>
      <title>Bayesian Multi-Logit Regression implemented in Tensorflow Probability</title>
      <link>https://quangtiencs.com/posts/bayesian_logistics_tensorflow_probability/</link>
      <pubDate>Sun, 20 Feb 2022 00:00:00 +0530</pubDate>
      
      <guid>https://quangtiencs.com/posts/bayesian_logistics_tensorflow_probability/</guid>
      <description>Bayesian Multi-Logit Regression is a probabilistic model for multiclass classification. This tutorial will make a prototype model in Tensorflow Probability and fit it with No-U-Turn Sampler.
Let&amp;rsquo;s start!
1. Model specification: Multi-Logit regression for \(K\) classes has the following form:
$$p(y | x, \beta) = \text{Categorical}(y| \text{softmax}(x \beta))$$
With:
\(x \): input features (row) vector \(x = [x_1,&amp;hellip; x_D] \in R^D \) \(y \): the predicted outcome of the class label \(\beta \): weight matrix for \(K\) classes and \(D\) dimensions The bayesian version of this model with pior:</description>
    </item>
    
  </channel>
</rss>
